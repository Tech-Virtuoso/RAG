
### âœ… Option 1: Use Startup Script *(Recommended)*

**Windows Command Prompt:**
```bash
start_app.bat
````

**PowerShell:**

```powershell
.\start_app.ps1
```

### ğŸ”§ Option 2: Manual Setup

1. **Activate Conda Environment:**

   ```bash
   conda activate chatbot
   ```

2. **Start Ollama Server (in separate terminal):**

   ```bash
   ollama serve
   ```

3. **Launch Flask Application:**

   ```bash
   python app.py
   ```

4. **Access in Browser:**

   ```
   http://localhost:5000
   ```

---

## ğŸ“¦ Prerequisites

* **Conda Environment:** `chatbot` (with all required packages)
* **Ollama Installed:** [https://ollama.ai/](https://ollama.ai/)
* **Required Models via Ollama:**

  * `nomic-embed-text` (embeddings)
  * `mistral` (text generation)

---

## ğŸ–¥ï¸ Ollama Server Requirements

### ğŸ” Why It Must Be Started Every Time

* Ollama **does not auto-start** as a service on Windows
* LLMs must be **loaded into memory** (Mistral: \~4.1 GB, Nomic: \~274 MB)
* **Flask app connects to Ollama** at `http://localhost:11434`
* Startup time varies depending on model load status

---

### âš™ï¸ Ollama Startup Options

#### ğŸŸ¢ Easy: Use Startup Scripts

These scripts will:

* Start Ollama in background
* Wait for models to load
* Launch Flask server

```bash
start_app.bat       # CMD
start_app.ps1       # PowerShell
```

#### ğŸŸ¡ Manual: Use Two Terminals

```bash
# Terminal 1
ollama serve

# Terminal 2
conda activate chatbot
python app.py
```

#### ğŸ”µ Optional: Windows Service Setup

To run Ollama automatically on Windows boot:

```bash
# Run once as Administrator
setup_ollama_service.bat
```

---

### ğŸ§  Model Load Times

| Scenario            | Duration  |
| ------------------- | --------- |
| First-time startup  | 10â€“30 sec |
| Subsequent startups | 5â€“15 sec  |
| Memory Usage        | \~4.5 GB  |

---

## âœ¨ Features

* ğŸ§  Conversational AI powered by **Mistral-7B**
* ğŸ“„ PDF support for contextual chat (e.g., CS224n notes)
* ğŸ” Semantic search via `nomic-embed-text`
* ğŸ’¬ Floating modern chat UI
* âš¡ Real-time query response
* ğŸ“Š Timestamped logs for monitoring

---

## ğŸ“ Project Structure

```
rag_chatbot_modular/
â”œâ”€â”€ app.py                      # Flask entry point
â”œâ”€â”€ chatbot.py                  # Core RAG logic
â”œâ”€â”€ vectorstore.py              # ChromaDB operations
â”œâ”€â”€ loaders.py                  # PDF/document loaders
â”œâ”€â”€ splitter.py                 # Text chunking logic
â”œâ”€â”€ llm_config.py               # Ollama model configuration
â”œâ”€â”€ logger_config.py            # Logging setup
â”œâ”€â”€ qa_chain.py                 # RAG chain logic
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html              # Web UI (chat interface)
â”œâ”€â”€ chroma_db/                  # Vector database storage
â”œâ”€â”€ logs/                       # Log files
â”œâ”€â”€ start_app.bat               # CMD startup script
â”œâ”€â”€ start_app.ps1               # PowerShell startup script
â”œâ”€â”€ setup_ollama_service.bat    # (Optional) Auto-start setup
â””â”€â”€ requirements.txt            # Python dependencies
```

---

## ğŸ› ï¸ Troubleshooting

### ğŸ”´ Ollama Server Not Running

* Ensure Ollama is installed: `https://ollama.ai/`
* Start Ollama: `ollama serve`
* Check available models: `ollama list`
* Pull models:

  ```bash
  ollama pull nomic-embed-text  
  ollama pull mistral
  ```

### ğŸ”´ Conda Environment Issues

* Create: `conda create -n chatbot python=3.9`
* Install dependencies: `pip install -r requirements.txt`

### ğŸ”´ Port 5000 Already in Use

* Modify the port in `app.py`
* Kill process using port:

  ```bash
  netstat -ano | findstr :5000  
  taskkill /PID <PID> /F
  ```

### ğŸ”´ Insufficient RAM

* Ensure your system has at least **8GB RAM** available

---

## ğŸ“œ Logs

* Located in `./logs/`
* Named with timestamps
* Used for debugging, monitoring, and auditing

---

## ğŸ§  Models Used

| Task            | Model                 | Source |
| --------------- | --------------------- | ------ |
| Embeddings      | `nomic-embed-text`    | Ollama |
| LLM             | `Mistral-7B-Instruct` | Ollama |
| Vector Database | `ChromaDB`            | Local  |
| PDF Parsing     | `PyPDF2`, LangChain   | Python |

---

## âœ… Summary

This RAG chatbot delivers a modern, modular, and production-ready pipeline for question answering over documents using Ollama and LangChain. With real-time semantic search and responsive UI, itâ€™s ideal for document understanding, study assistance, or custom enterprise chatbots.

---

## ğŸ“¬ Questions or Feedback?

Feel free to open an [issue](https://github.com/your-repo/issues) or contact the maintainer for support.

---

